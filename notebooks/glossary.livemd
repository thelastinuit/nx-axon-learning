# Glossary of ML Terms

## Goal

A glossary of frequently used terms in Machine Learning to reference throughout the training.

## Glossary

<details>
  <summary>Accuracy</summary>
  the predictions a classification model got right.
</details>

<details>
  <summary>Activation Function</summary>
  takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value to the next layer (i.e. ReLU or sigmoid)
</details>

<details>
  <summary>Anomaly Detection</summary>
  identifying outliers, like if the mean for one feature is 100 with a standard deviation of 10, then an outlier is a value of 200.
</details>

<details>
  <summary>Artificial Intelligence</summary>
  non-human program or model able to solve complex tasks. Machine learning is a type of artificial intelligence.
</details>

<details>
  <summary>Backpropagation</summary>
  gradient descent algorithm on neural networks performed after each forward pass through the network to adjust the modelâ€™s weights.
</details>

<details>
  <summary>Batch</summary>
  set of examples used in one iteration of model training; batch size refers to the number of examples in the batch.
</details>

<details>
  <summary>Bias</summary>
  an additional input of a neural network with a constant value, positive or negative, used to increase or decrease the output of a neuron. It helps to ensure that even if all the inputs are zeros there will still be an activation in the next layer. For example, in a sigmoid function, the biash shifts the curve left or right but does not change the same of the curve.
</details>

<!-- livebook:{"break_markdown":true} -->

<!-- Learn more at https://mermaid-js.github.io/mermaid -->

```mermaid
graph LR;
  Biases([Biases])-->Weights;
  Training_Inputs[fa:fa-database Training Inputs]-->Weights;
  Weights([Weights])-->Outputs;
  Ground_Truth([Ground Truth])-->Loss{Loss};
  Outputs[Outputs]-->Loss;
  Loss--Backpropagation-->Weights;
```

<!-- livebook:{"break_markdown":true} -->

<details>
  <summary>Class</summary>
  the "class" predicted in classification machine learning. For example, if you're attempting to detect fraud the classes would be "fraud" and "not fraud". In the case of multiple classes to identify dog breeds the classes are pug, golden retriever, poodle, etc.
</details>

<details>
  <summary>Classification</summary>
  separating data into multiple categorical classes or discrete values. Compared to regression, classification results in unordered discrete values by measuring data.
</details>

<details>
  <summery>Cross Entropy Loss (softmax)</summary>
  Used in cLassification algorithms typically to measure loss as the difference between probabilities that a model assigns to classes.
</details>

<details>
  <summary>Data</summary>
  (Dataset) collection of examples
</details>

<details>
  <summary>Dataframe</summary>
  datatype analogous to a table where each column of the dataFrame has a header.
</details>

<details>
  <summary>Epoch</summary>
  full training iteration over the dataset where example has been seen once
</details>

<details>
  <summary>Ensemble Learning</summary>
  collection of models trained independently whose predictions are averaged or aggregated.
</details>

<!-- livebook:{"break_markdown":true} -->

![](images/classification.png)

<!-- livebook:{"break_markdown":true} -->

<details>
  <summary>Feature</summary>
  independent input variable; for prediction the risk of heart disease, the features might be age, gender, and weight.
</details>

<details>
  <summary>Generalization</summary>
  model's ability to accurately predict outcomes on previously unseen data.
</details>

<details>
  <summary>Hyperparameter</summary>
  the parameter whose value is fine tuned during training iterations of a model to control the learning process; i.e. learning rate. In contrast to a parameter, a hyper parameter is external to the model and its value cannot be estimated from the data.
</details>

<details>
  <summary>Imbalanced Dataset</summary>
  a dataset in which the labels for the two classes in a binary classification problem occur in significantly different frequencies. An example would be a dataset meant to train a model to predict disease where 1% of the examples are positive and 99% are negative.
</details>

<details>
  <summary>Iteration</summary>
  single update of a model's weights during training.
</details>

<!-- livebook:{"break_markdown":true} -->

<!-- Learn more at https://mermaid-js.github.io/mermaid -->

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor':'#800080', 'secondaryColor':'#eabfff'}}}%%
pie showData
    title Imbalanced Dataset
    "Negative" : 1.01
    "Positive" :  98.99
```

<!-- livebook:{"break_markdown":true} -->

<details>
  <summary>Label</summary>
  (Target) the outcome "result" column of an example in supervised learning. In fraud detection the label would be "fraud" or "not fraud".
</details>

<details>
  <summary>Learning Rate</summary>
  parameter that determines the size of the optimization step at each iteration. It affects how quickly a model can learn because it directs how information learned by the loss function overrides old information.
</details>

<details>
  <summary>Loss</summary>
  measure of the distance between a model's prediction from its label.
</details>

<details>
  <summary>Loss Function</summary>
  reveals the accuracy of a model; the result of the loss function is the distance between the prediction and the true value. Its indicates whether or not weights and biases should be adjusted to improve accuracy. It is an example of a machine learning algorithm learning from its mistakes.
</details>

<!-- livebook:{"break_markdown":true} -->

| Labeled Data                                   | Unlabeled Data                         |
| ---------------------------------------------- | -------------------------------------- |
| Supervised learning                            | Unsupervised Learning                  |
| Labels made through observation and collection | Human must label                       |
| Most often used to preprocess datasets         | Most often used for complex prediction |

<!-- livebook:{"break_markdown":true} -->

<details>
  <summery>Mean Squared Error (MSE)</summary>
  Used in regression algorithms to determine loss as the average squared difference between the prediction and the ground truth.
</details>

<details>
  <summary>Mini-batch</summary>
  a small subset of the entire batch of examples run together in a single training iteration.
</details>

<details>
  <summary>Model</summary>
  file representing what a machine learning system learned from the training data.
</details>

<details>
  <summary>Normalization</summary>
   process of converting values into a standard range typically 0 to 1.
</details>

<details>
  <summary>Optimizer</summary>
  specific implementation of the gradient descent algorithm; i.e. Adam (ADAptive with Momentum).
</details>

<!-- livebook:{"break_markdown":true} -->

<details>
  <summary>Parameter</summary>
  variable that is internal to the model and whose value can be estimated from data; compare to a hyperparameter.
</details>

<details>
  <summary>Precision</summary>
  classification model metric to identify the frequency of how often the model predicted accurately.
</details>

<details>
  <summary>Prediction</summary>
  model's output when provided an input.
</details>

<details>
  <summary>Recall</summary>
  classification model metric of how many out of all the possible positive labels did the model accurately classify?
</details>

<details>
  <summary>Regression</summary>
  distinguishing data into continuous real values rather than classes or discrete values. Compared to classification, regression results in ordered continuous values by measuring the root mean square error.
</details>

<!-- livebook:{"break_markdown":true} -->

![](images/prediction.png)

<!-- livebook:{"break_markdown":true} -->

<details>
  <summary>Shape</summary>
  number of elements in each dimension of a tensor.
</details>

<details>
  <summary>Sigmoid Function</summary>
  maps logistic or multinomial regression output to probabilities, returning a value between 0 and 1.
</details>

<details>
  <summary>Target</summary>
  (Label) the outcome "result" column of an example in supervised learning. In fraud detection the label would be "fraud" or "not fraud".
</details>

<details>
  <summary>Tensor</summary>
  data structure which is a multidimensional array of vectors and matrices. A vector is a one-dimensional tensor and a matrix is a two-dimensional tensor.
</details>

<details>
  <summary>Training</summary>
  process of determining the ideal parameters to make up a model.

</details>

<details>
  <summary>Weights</summary>
  enable a neural network to increase or decrease connections between neurons to increase the influence of one over another.
</details>

<!-- livebook:{"break_markdown":true} -->

![](images/sigmoid%20function.png)
