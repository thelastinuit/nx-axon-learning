# Machine Learning / Introduction

```elixir
Mix.install([
  {:gibran, git: "https://github.com/abitdodgy/gibran"}
])
```

## Goal

This lesson serves as an introduction to and an overview of machine learning.

<!-- livebook:{"break_markdown":true} -->

### Introduction

Machine Learning (ML) is a type of Artificial Intelligence (AI) and is the process by which machines improve their performance without explicit programming. Machines discover patterns, are able to make predictions and get better over time with exposure to data. In some types of machine learning these predictions can become increasingly accurate over time without human intervention. Historical data is given as training input. Through training and testing, the machine learning algorithms are able to predict new, unseen output values.

<!-- livebook:{"break_markdown":true} -->

#### Common Uses

* classification of images
* recommendation engines
* fraud and threat detection
* business process automation

<!-- livebook:{"break_markdown":true} -->

![breeds](images/breeds.png)

<!-- livebook:{"break_markdown":true} -->

### Types of Machine Learning

Machine learning types are broken down by how an algorithm learns prediction accuracy. The practice of machine learning means knowing the right question to ask and how to choose the algorithm based on the type of data you want to predict.

There are three main types of machine learning

* Supervised learning
* Unsupervised learning
* Reinforcement learning

We will cover each of them in the next section.

<!-- livebook:{"break_markdown":true} -->

### How it Works

The goal of ML is to make a **prediction**. Once a prediction is made it is compared to the **ground truth** (the expected result).

The **loss function** gives a quantity to the distance between the prediction and the ground truth.

Here is a sample worksflow of a supervised learning system:

<!-- livebook:{"break_markdown":true} -->

```mermaid
flowchart TD;
  ML{{Supervised Machine Learning}}-->TRN([TRAINING]);
  ML-->TST[TESTING];
  TRN-->In([Input training dataset]);
  In-->LS([Learning System]);
  In---GT([Ground Truth]);
  LS-->Out([Verifiable Output]);
  Out-->M((Model));
  GT-->Out;
  TST-->UD[New, unseen input data];
  UD-->M;
  M-->P[[Prediction, a.k.a best guess]]
  style ML fill:#B7D7EB,stroke:#000,color:#000,stroke-width:2px
  style TRN fill:#aaa,stroke:#000,color:#000,stroke-width:4px
  style In fill:#fff,stroke:#000,color:#000,stroke-width:2px, stroke-dasharray: 5 5
  style TST fill:#aaa,stroke:#019EFE,color:#000,stroke-width:4px
  style UD fill:#fff,stroke:#019EFE,color:#000,stroke-width:2px, stroke-dasharray: 5 5
  style M fill:#bbf,stroke:#808,color:#000,stroke-width:4px
  style P fill:#fff,stroke:#808,color:#000,stroke-width:4px
  style GT stroke-dasharray: 5 5
```

<!-- livebook:{"break_markdown":true} -->

```mermaid
graph TD;
  ML([Machine Learning])-->PD([Prediction]);
  PD-->LF([Loss Function]);
  GT([Ground Truth])-->LF;
  LF-->RG([Regression]);
  LF-->CL([Classification]);
  RG-->MSE([Mean Squared Error]);
  CL-->CEL([Cross Entropy Loss]);
```

<!-- livebook:{"break_markdown":true} -->

### ML Algorithms

An algorithm is a mathematical process upon which following specific steps lead to an expected outcome. In Machine Learning, algorithms draw upon linear algebra, statistics and calculus to weigh the probability of multiple outcomes. They can be implemented in a variety of different languages.

Essentially, ML algorithms are functions with an input of data and an output of predictive models.

Examples of well-known algorithms are:

* Regression (linear, logistic, etc.)
* Classification
* Clustering
* Decision Trees
* Random Forest
* k-Nearest Neighbors
* Artificial neural networks (ANN)

<!-- livebook:{"break_markdown":true} -->

Most supervised machine learning is either **classification** or **regression**.

Classification algorithms typically use Cross Entropy Loss (aka softmax), which measures loss as the difference between probabilities that a model assigns to classes.

Regression algorithms typically use Mean Square Error (MSE), which determines loss as the average squared difference between the prediction and the ground truth.

<!-- livebook:{"break_markdown":true} -->

![](images/decision_tree.png)

<span style="float: right; font-size: 12px;">
_Decision Tree_
</span>

<!-- livebook:{"break_markdown":true} -->

### Models

If a machine learning model is the output of an algorithm then it is effectively a file. This file has been trained to recognize patterns by a set of data. An algorithm is used to reason over and learn from this data. During training, the algorithm is optimized to find certain patterns in the dataset resulting in a file called a machine learning model.

The machine learning model represents what's been learned from the algorithm using a training dataset through the prediction algorithm it produces; an algorithm like linear regression will produce a model made of a vector of coefficients with specific values whereas a decision tree algorithm will result in a model made of a tree of conditions with specific values.

Once trained, the model can reason over and make predictions upon data it hasn't seen before. If the model has been trained well these predictions can be accurate.

Examples include machine learning models that can be used for natural language processing (NLP) which recognize the intent or sentiment of combinations of words. Another is image recognition; machine learning models can learn to recognize and discern objects.

## Model Training

Model training is the process of running a machine learning algorithm on a dataset and optimizing the algorithm to find patterns. The data set is referred to as the training data. The result is the model; a function made of rules and data structures.

To train a model, data scientists split their dataset into two categories: "in sample" and "out of sample". The split is generally 70% training, 30% testing. The model is trained on the in sample training set and then tested on how accurately it can make predictions on the out of sample testing set.

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 15.0px;" />

## Brain Break!

<span style="color: #800080; font-size: 24px; font-weight: bold;">
Time for some Elixir
</span>

<img src="https://static.thenounproject.com/png/1111032-200.png" alt="Brain icon" style="width: 85px; float: left; margin-right: 40px;" />

Let's get some fingers on the keyboard and explore an Elixir library.

[Gibran](https://github.com/abitdodgy/gibran) is an early Elixir library that experimented with natural language processor. It can convert a string into a list of tokens using different strategies.

```elixir
str = "Tomorrow, and tomorrow, and tomorrow, Creeps in this petty pace from day to day"

Gibran.Tokeniser.tokenise(str)
|> IO.inspect(label: "tokens")

Gibran.Tokeniser.tokenise(str)
|> Gibran.Counter.uniq_tokens()
|> IO.inspect(label: "unique tokens")

Gibran.Tokeniser.tokenise(str, exclude: &(String.length(&1) < 5))
|> IO.inspect(label: "tokens > 10")

Gibran.Tokeniser.tokenise(str, exclude: &(String.length(&1) < 5))
|> Enum.uniq()
|> IO.inspect(label: "unique tokens > 10")

#### Challenge

How could this library be of use in your ML tasks? 

:ok
```

<hr style="background-color: #800080;height: 15.0px;" />

## Problems in Machine Learning

<hr style="background-color: #EED202; height: 5px;" />

<span style="width: 30px; float: left; margin-right: 30px;">

</span>

![](images/caution.png)

<span style="font-size: 16px; font-style: italic;">
It is easy to think that more data is better because more data generally leads to increased accuracy but it also leads to decreased performance and difficulty in visualizing the data.
</span>

<hr style="background-color: #EED202;height: 5px;" />

<!-- livebook:{"break_markdown":true} -->

### Generalization

**Generalization** refers to the generalizability of a machine learning model. In otherwords, generalization is the ability of the model to make accurate preditions on data it has never seen before. This is what makes machine learning useful in solving everyday problems that require predictions or classification.

Say you train a model to classify images into two groups: "dogs" or "not dogs". If the dataset you used to train the model contains only two breeds of dogs it may perform very accurately. Will it generalize to a dataset containing more dog breeds? Or one with cats and wolves? Unlikely. In this case you might see the testing dataset with 90% accuracy. That accuracy is likely to drop to the 70s for the other dog breeds and below 40 for the mix of animals.

The problem in this example is a lack of data diversity. Let's look at two other problems we can run into with training data: overfitting and underfitting.

<!-- livebook:{"break_markdown":true} -->

![](images/fit.png)

<!-- livebook:{"break_markdown":true} -->

### Overfitting

Overfitting occurs when a model too closely matches, or fits, it’s in-sample training data. This means the model will not generalize; it will not perform well on out-of-sample data. This can happen if the model overtrains on a dataset, if hyperparameters are aggressively tuned, or any number of factors that can cause a model to view a dataset as the only accurate depiction of the world.

For example, the image below shows the error rate of a decision tree on both in-sample and out-of-sample-data. Notice that as the number of nodes in a leaf increases the better our prediction on both datasets becomes. However, around a node count of 10 our in-sample error rate decreases while our error rate on the out-of-sample increases. It is at this point in the graph where we know that we’re overfitting our model – essentially, we’re becoming great at predicting data in our training set but poorly predicting data in our test set.

<!-- livebook:{"break_markdown":true} -->

![](images/overfitting.png)

<!-- livebook:{"break_markdown":true} -->

### Underfitting

Underfitting is when a model produces inaccurate prediction outcomes because it failed to capture the complexity of the training data. It occurs when the model has not trained for enough time or the input variables are not significant enough to form a meaningful relationship between the input and output. Underfitting, like overfitting, generalizes poorly to out of sample data.

![](images/underfit.png)

<!-- livebook:{"break_markdown":true} -->

![](images/underfit.png)

<!-- livebook:{"break_markdown":true} -->

[<span style="float: left; color: #800080; font-weight: bold; font-family: FreeMono, monospace">< previous</span>](../toc.livemd)

[<span style="float: right; color: #800080; font-weight: bold; font-family: FreeMono, monospace">next ></span>](2_Supervised.livemd)
